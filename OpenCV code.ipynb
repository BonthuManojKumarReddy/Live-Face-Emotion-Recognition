{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code in  Local Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-cfe86a7d5412>:14: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if face_roi is ():                                            # checking if face_roi is empty that is if no face detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000222E03D71F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000222E0451940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "face_detect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  \n",
    "\n",
    "    \n",
    "\n",
    "def face_detection(img,size=0.5):\n",
    "    \n",
    "    # converting image into grayscale image\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Region Of Interest of detected face\n",
    "    face_roi = face_detect.detectMultiScale(img_gray, 1.3,1)   \n",
    "    \n",
    "    # Labels for emotion detected \n",
    "    class_labels = ['Fear','Angry','Neutral','Happy']\n",
    "                                                              \n",
    "     # checking: If face_roi is empty i.e, if no face detected then return empty image\n",
    "    if face_roi is ():                                           \n",
    "        return img\n",
    "    \n",
    "    # Iterating over each face identified and drawing rectangle\n",
    "    for(x,y,w,h) in face_roi:                                     \n",
    "        x = x - 5\n",
    "        w = w + 10\n",
    "        y = y + 7\n",
    "        h = h + 2\n",
    "        \n",
    "        # (x,y)- top left point , (x+w,y+h)-bottom right point ,(0,255,255)-colour of the rectangle, 1-thickness\n",
    "        cv2.rectangle(img, (x,y),(x+w,y+h),(0,255,255), 2)  \n",
    "        \n",
    "        # Croping gray scale image \n",
    "        img_gray_crop = img_gray[y:y+h,x:x+w]  \n",
    "        # Croping color image\n",
    "        img_color_crop = img[y:y+h,x:x+w]                        \n",
    "        \n",
    "        # Loading the model with best parameters\n",
    "        model=load_model('model_2bestweights.h5')\n",
    "        # Size of colured image is resized to (48,48)\n",
    "        final_image = cv2.resize(img_color_crop, (48,48),interpolation=cv2.INTER_AREA)  \n",
    "        # Array is expanded by inserting axis at position 0\n",
    "        final_image = np.expand_dims(final_image, axis = 0)    \n",
    "        # Feature scaling of final image\n",
    "        final_image = final_image/255.0    \n",
    "        \n",
    "        # Predicting emotion of the captured image from the trained model\n",
    "        prediction = model.predict(final_image)  \n",
    "        # Finding the label of the class which has maximaum probalility\n",
    "        label=class_labels[prediction.argmax()]                    \n",
    "        cv2.putText(frame,label, (30,80), cv2.FONT_HERSHEY_SIMPLEX,2, (18,10,200),1)  \n",
    "                                                                 \n",
    "\n",
    "     # Fliping the image\n",
    "    img_color_crop = cv2.flip(img_color_crop, 1)                 \n",
    "    return img\n",
    "\n",
    "# Capturing the video from live webcam\n",
    "cap = cv2.VideoCapture(0)                                         \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    # Captured frame will be sent to face_detection function for emotion detection\n",
    "    cv2.imshow('LIVE', face_detection(frame))                     \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
